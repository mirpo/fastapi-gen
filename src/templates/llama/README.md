<div align="center">

# ğŸ¦™ Llama - Local LLM Powerhouse Template

**Production-ready FastAPI with local LLM inference using llama-cpp-python**

*This project was bootstrapped with [FastAPI Gen](https://github.com/mirpo/fastapi-gen)*

[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![llama-cpp-python](https://img.shields.io/badge/llama--cpp--python-Latest-blue.svg)](https://github.com/abetlen/llama-cpp-python)
[![GGUF](https://img.shields.io/badge/GGUF-Format-orange.svg)](https://huggingface.co/docs/hub/gguf)

</div>

---

## ğŸ¯ What You'll Build

A **powerful local LLM inference service** with enterprise-grade features:

ğŸ  **Local LLM Focus** â†’ Optimized for Gemma/Llama GGUF models  
âš¡ **GPU Acceleration** â†’ Auto GPU detection, configurable layers  
ğŸ›ï¸ **Advanced Config** â†’ Context windows, threading, performance tuning  
ğŸ—ï¸ **Production Ready** â†’ Lifecycle management, health monitoring  
ğŸ§ª **Real Testing** â†’ Actual model inference validation  
ğŸ”§ **Easy Setup** â†’ Auto model download, optimized defaults  

## âš¡ Quick Start in 30 Seconds

```bash
# You're already here! First initialize the environment:
make init  # Downloads model (~135MB), sets up environment

# Then start the app:
make start

# Open: http://localhost:8000/docs
```

ğŸš€ **Open:** [http://localhost:8000/docs](http://localhost:8000/docs) to see your interactive API documentation.

> ğŸ”„ **First time setup:** `make init` downloads the model and sets up everything (~2-3 minutes).

## ğŸŒŸ Local LLM Features

<details>
<summary><strong>ğŸ  Optimized Local LLM Inference</strong></summary>

**Purpose-built for local models:**
- âœ… **GGUF Format Support** - Optimized for quantized Gemma/Llama models
- âœ… **Startup Model Loading** - Models load once during app startup
- âœ… **Memory Efficient** - Reuses Llama model instances across requests
- âœ… **GPU Acceleration** - Automatic GPU detection and configurable layers
- âœ… **Multi-threading** - Configurable thread usage for optimal performance

**Model Optimizations:**
- Quantized models for speed and memory efficiency
- Smart memory management across requests
- Configurable context windows for longer conversations

</details>

<details>
<summary><strong>âš¡ Advanced Configuration</strong></summary>

**Fine-grained performance control:**
- âœ… **Context Window Control** - Configurable n_ctx for longer conversations
- âœ… **GPU Layer Management** - Control how much runs on GPU vs CPU
- âœ… **Thread Optimization** - CPU thread configuration for best performance
- âœ… **Memory Control** - Smart memory allocation and management
- âœ… **Seed Control** - Reproducible outputs with configurable random seed

**Performance Tuning:**
```bash
LLM_N_CTX=2048           # Context window size
LLM_N_THREADS=-1         # CPU threads (-1 = auto)
LLM_N_GPU_LAYERS=-1      # GPU layers (-1 = auto)
LLM_SEED=-1              # Random seed (-1 = random)
```

</details>

<details>
<summary><strong>ğŸ—ï¸ Production-Ready Architecture</strong></summary>

**Enterprise-grade service management:**
- âœ… **Lifecycle Management** - Proper startup/shutdown with async context managers
- âœ… **Health Monitoring** - `/health` endpoint with detailed model information
- âœ… **Structured Logging** - Comprehensive logging with proper levels
- âœ… **Error Handling** - Graceful error responses with detailed messages
- âœ… **Service Initialization** - Robust model loading with error recovery

**Service Features:**
- Automatic model validation on startup
- Graceful degradation and error recovery
- Real-time service status monitoring
- Resource usage tracking

</details>

<details>
<summary><strong>ğŸ›ï¸ Modern API Design</strong></summary>

**Flexible endpoint architecture:**
- âœ… **Dual Endpoints** - Both GET and POST (modern) endpoints
- âœ… **Request Validation** - Pydantic models with type-safe input validation
- âœ… **Response Models** - Structured responses with model metadata and token usage
- âœ… **Parameter Customization** - Override generation parameters per request
- âœ… **Constraint Validation** - Proper limits and validation rules

**API Patterns:**
- Modern: `POST /question-answering` (recommended)
- Legacy: `GET /question-answering?question=...` (for quick testing)

</details>

<details>
<summary><strong>ğŸ§ª Real Model Testing</strong></summary>

**Comprehensive validation approach:**
- âœ… **Session-scoped Fixtures** - Efficient test execution with shared model loading
- âœ… **Real Service Testing** - Tests use actual model inference, not mocks
- âœ… **Comprehensive Coverage** - All endpoints, parameters, validation, edge cases
- âœ… **Performance Validation** - Ensures real-world functionality works correctly
- âœ… **Hardware Testing** - Validates GPU/CPU configurations

**Testing Philosophy:**
- Tests actual llama-cpp-python integration
- Validates model loading and inference pipelines
- Ensures API contracts match real model behavior
- Provides confidence for production deployment

</details>

<details>
<summary><strong>ğŸ”§ Easy Setup & Management</strong></summary>

**Streamlined development experience:**
- âœ… **Auto Model Download** - Automatic model fetching with make commands
- âœ… **Optimized Defaults** - Sensible default configurations
- âœ… **Environment Detection** - Auto GPU/CPU detection and configuration
- âœ… **Error Recovery** - Robust handling of model loading failures
- âœ… **Development Tools** - Makefile with common development commands

**Setup Features:**
- One-command environment initialization
- Automatic dependency management
- Smart hardware detection and optimization

</details>

## ğŸ“¡ API Endpoints

### ğŸ¥ Health & Status
```http
GET /health                 # Service status, model info, GPU config, threads
```

### ğŸ¤– Question Answering
```http
# Modern POST endpoint (recommended)
POST /question-answering
Content-Type: application/json

{
  "question": "What is the difference between AI and machine learning?",
  "max_tokens": 150,
  "temperature": 0.7
}

# Legacy GET endpoint (quick testing)
GET /question-answering?question=What%20is%20AI&max_tokens=100&temperature=0.5
```

## ğŸ› ï¸ Development Commands

<details>
<summary><strong>Available Make Commands</strong></summary>

| Command | Description |
|---------|-------------|
| `make init` | ğŸš€ Download model and set up environment |
| `make start` | â–¶ï¸ Run app in development mode with auto-reload |
| `make test` | ğŸ§ª Run comprehensive test suite with real AI |
| `make lint` | ğŸ” Run code quality checks with Ruff |
| `make download` | ğŸ“¥ Download model file only (auto-detects curl/wget) |

</details>

## ğŸ”§ Configuration & Setup

### Environment Configuration

Create `.env_dev` file:
```bash
# Model configuration
LLM_MODEL="./models/SmolLM2-135M-Instruct-Q4_K_M.gguf"

# Generation parameters
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=100

# Performance tuning
LLM_N_CTX=2048           # Context window size
LLM_N_THREADS=-1         # CPU threads (-1 = auto)
LLM_N_GPU_LAYERS=-1      # GPU layers (-1 = auto, 0 = CPU only)
LLM_VERBOSE=true
LLM_SEED=-1              # Random seed (-1 = random)
```

### Performance Configuration Guide

| Parameter | Description | Default | Notes |
|-----------|-------------|---------|-------|
| `LLM_N_CTX` | Context window size | 2048 | Larger values use more memory |
| `LLM_N_THREADS` | CPU threads | -1 (auto) | Set to number of CPU cores |
| `LLM_N_GPU_LAYERS` | GPU layers | -1 (auto) | 0 = CPU only, higher = more GPU |
| `LLM_SEED` | Random seed | -1 (random) | Set for reproducible outputs |

## ğŸ”§ Model Management

### Default Model: SmolLM2 135M Instruct

**Optimized for development and learning:**
- **Size:** ~135MB download (Q4_K_M quantized)
- **Memory:** ~200MB RAM usage
- **Performance:** Very fast inference on CPU
- **Quality:** Compact model optimized for efficiency

### Using Different Models

1. **Download your GGUF model** to `./models/` directory
2. **Update configuration:**
   ```bash
   LLM_MODEL="./models/your-model.gguf"
   ```
3. **Adjust settings** for your model:
   ```bash
   LLM_N_CTX=4096        # If your model supports larger context
   LLM_N_GPU_LAYERS=32   # Adjust based on your GPU memory
   ```

### Popular Model Options

| Model | Size | Memory | Use Case |
|-------|------|--------|----------|
| SmolLM2-135M | ~135MB | ~200MB | Development, fast inference |
| SmolLM2-360M | ~360MB | ~500MB | Better quality, still fast |
| Llama-3.2-1B | ~1GB | ~1.5GB | Production quality |
| Llama-3.2-3B | ~3GB | ~4GB | High quality responses |

## ğŸ›ï¸ Hardware Acceleration

### GPU Support

**Automatic Detection:**
- âœ… **CUDA** - NVIDIA GPUs automatically detected
- âœ… **Metal** - Apple Silicon Macs automatically detected  
- âœ… **CPU Fallback** - Graceful fallback to CPU-only mode

**GPU Configuration:**
```bash
# Auto-detect and use all available GPU layers
LLM_N_GPU_LAYERS=-1

# Use specific number of GPU layers
LLM_N_GPU_LAYERS=20

# CPU-only mode
LLM_N_GPU_LAYERS=0
```

### Performance Optimization

**For CPU-only systems:**
```bash
LLM_N_THREADS=8          # Set to your CPU core count
LLM_N_GPU_LAYERS=0       # Disable GPU
LLM_N_CTX=1024          # Smaller context for less memory
```

**For GPU systems:**
```bash
LLM_N_GPU_LAYERS=-1      # Use all GPU layers
LLM_N_CTX=4096          # Larger context with GPU
LLM_N_THREADS=4         # Fewer CPU threads when using GPU
```

## ğŸ§ª Testing Strategy

**Real model validation:**

```bash
make test
# Tests use actual model inference
# First run initializes models (takes longer)
# Subsequent runs are fast (cached models)
```

**What gets tested:**
- âœ… Model loading and initialization
- âœ… Question answering with various parameters
- âœ… Error handling for invalid inputs
- âœ… GPU/CPU configuration validation
- âœ… Context window and threading
- âœ… API contract validation

## ğŸ“ Project Structure

```
llama/
â”œâ”€â”€ main.py              # FastAPI app with llama-cpp-python
â”œâ”€â”€ models/
â”‚   â””â”€â”€ SmolLM2-135M-Instruct-Q4_K_M.gguf  # Downloaded model
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_main.py     # Real AI integration tests
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ requirements.txt     # Dependencies (llama-cpp-python, etc.)
â”œâ”€â”€ .env_dev            # Environment configuration
â”œâ”€â”€ Makefile            # Development commands
â””â”€â”€ README.md           # This file
```

## ğŸ“ Local LLM Learning Path

### Mastering Local LLM Inference

1. **ğŸ”§ Setup & Configuration**
   - Run `make init` to set up everything
   - Check `/health` endpoint for model and hardware info
   - Experiment with different configuration parameters

2. **ğŸ¤– Test Question Answering**
   - Start with simple questions via GET endpoint
   - Move to POST endpoint with custom parameters
   - Experiment with temperature and token limits

3. **âš¡ Performance Tuning**
   - Monitor memory usage during inference
   - Test CPU vs GPU performance
   - Optimize thread and layer configurations

4. **ğŸ”„ Model Experimentation**
   - Try different GGUF models
   - Compare model sizes vs quality
   - Test context window capabilities

5. **ğŸ§ª Run Real Tests**
   - Execute `make test` to see model validation
   - Study test patterns for your own endpoints
   - Understand model loading and inference strategies

## ğŸš€ Production Deployment

### ğŸ”’ Security Best Practices

- [ ] Implement API key authentication
- [ ] Add rate limiting for inference endpoints
- [ ] Validate and sanitize all text inputs
- [ ] Set up request/response logging
- [ ] Configure CORS for specific domains
- [ ] Monitor model usage and resource consumption

### âš¡ Performance Optimization

- [ ] Use appropriate model quantization (Q4_0, Q5_0, Q8_0)
- [ ] Configure optimal GPU layers for your hardware
- [ ] Set thread count to match CPU cores
- [ ] Balance context window with memory constraints
- [ ] Implement request batching for efficiency
- [ ] Add model response caching for common queries

### ğŸ› ï¸ Scaling Considerations

- [ ] Plan for model file distribution and caching
- [ ] Set up multiple model instances for load balancing
- [ ] Configure monitoring for memory and GPU usage
- [ ] Implement model warm-up procedures
- [ ] Plan for model updates and versioning
- [ ] Set up backup inference endpoints

## ğŸ”„ Extension Ideas

<details>
<summary><strong>ğŸ¤– Advanced LLM Features</strong></summary>

**Ready to implement:**
- Multi-turn conversation with context memory
- Custom prompt templates and formatting
- Function calling and tool usage
- JSON schema-constrained outputs
- Streaming responses for long generations

</details>

<details>
<summary><strong>ğŸ—ï¸ Infrastructure Scaling</strong></summary>

**Production enhancements:**
- Multiple model hosting with model switching
- Load balancing across multiple instances
- Model quantization optimization
- Custom sampling strategies
- Integration with monitoring systems

</details>

<details>
<summary><strong>ğŸ”§ Model Management</strong></summary>

**Advanced model operations:**
- Automatic model downloading and caching
- Model validation and testing pipelines
- A/B testing between different models
- Model performance benchmarking
- Custom fine-tuned model integration

</details>

## ğŸš€ Next Steps

### Explore Other AI Templates

- ğŸ¤– **Comprehensive NLP** - Try the [NLP template](../nlp/README.md) for 8 NLP capabilities
- ğŸ”— **LangChain Integration** - Check out [LangChain template](../langchain/README.md) for workflows
- ğŸš€ **Enterprise Features** - Add auth with [Advanced template](../advanced/README.md)

### Learn More About Local LLMs
- ğŸ“š [llama-cpp-python Documentation](https://github.com/abetlen/llama-cpp-python)
- ğŸ¦™ [Llama Model Family](https://ai.meta.com/llama/)
- ğŸ”§ [GGUF Format Guide](https://huggingface.co/docs/hub/gguf)
- âš¡ [Quantization Explained](https://huggingface.co/blog/4bit-transformers-bitsandbytes)

---

<div align="center">

**Local LLM power with production-ready patterns** ğŸ¦™

*Want cloud-based LLMs? Try the [ğŸ”— LangChain template](../langchain/README.md)*

</div>