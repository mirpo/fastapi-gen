<div align="center">

# ü¶ô Llama - Local LLM Powerhouse Template

**Production-ready FastAPI with local LLM inference using llama-cpp-python**

*This project was bootstrapped with [FastAPI Gen](https://github.com/mirpo/fastapi-gen)*

[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![llama-cpp-python](https://img.shields.io/badge/llama--cpp--python-Latest-blue.svg)](https://github.com/abetlen/llama-cpp-python)
[![GGUF](https://img.shields.io/badge/GGUF-Format-orange.svg)](https://huggingface.co/docs/hub/gguf)

</div>

---

## üéØ What You'll Build

A **powerful local LLM inference service** with enterprise-grade features:

üè† **Local LLM Focus** ‚Üí Optimized for Gemma/Llama GGUF models  
‚ö° **GPU Acceleration** ‚Üí Auto GPU detection, configurable layers  
üéõÔ∏è **Advanced Config** ‚Üí Context windows, threading, performance tuning  
üèóÔ∏è **Production Ready** ‚Üí Lifecycle management, health monitoring  
üß™ **Real Testing** ‚Üí Actual model inference validation  
üîß **Easy Setup** ‚Üí Auto model download, optimized defaults  

## ‚ö° Quick Start in 30 Seconds

```bash
# You're already here! First initialize the environment:
make init  # Downloads model (~135MB), sets up environment

# Then start the app:
make start

# Open: http://localhost:8000/docs
```

üöÄ **Open:** [http://localhost:8000/docs](http://localhost:8000/docs) to see your interactive API documentation.

> üîÑ **First time setup:** `make init` downloads the model and sets up everything (~2-3 minutes).

## üåü Local LLM Features

<details>
<summary><strong>üè† Optimized Local LLM Inference</strong></summary>

**Purpose-built for local models:**
- ‚úÖ **GGUF Format Support** - Optimized for quantized Gemma/Llama models
- ‚úÖ **Startup Model Loading** - Models load once during app startup
- ‚úÖ **Memory Efficient** - Reuses Llama model instances across requests
- ‚úÖ **GPU Acceleration** - Automatic GPU detection and configurable layers
- ‚úÖ **Multi-threading** - Configurable thread usage for optimal performance

**Model Optimizations:**
- Quantized models for speed and memory efficiency
- Smart memory management across requests
- Configurable context windows for longer conversations

</details>

<details>
<summary><strong>‚ö° Advanced Configuration</strong></summary>

**Fine-grained performance control:**
- ‚úÖ **Context Window Control** - Configurable n_ctx for longer conversations
- ‚úÖ **GPU Layer Management** - Control how much runs on GPU vs CPU
- ‚úÖ **Thread Optimization** - CPU thread configuration for best performance
- ‚úÖ **Memory Control** - Smart memory allocation and management
- ‚úÖ **Seed Control** - Reproducible outputs with configurable random seed

**Performance Tuning:**
```bash
LLM_N_CTX=2048           # Context window size
LLM_N_THREADS=-1         # CPU threads (-1 = auto)
LLM_N_GPU_LAYERS=-1      # GPU layers (-1 = auto)
LLM_SEED=-1              # Random seed (-1 = random)
```

</details>

<details>
<summary><strong>üèóÔ∏è Production-Ready Architecture</strong></summary>

**Enterprise-grade service management:**
- ‚úÖ **Lifecycle Management** - Proper startup/shutdown with async context managers
- ‚úÖ **Health Monitoring** - `/health` endpoint with detailed model information
- ‚úÖ **Structured Logging** - Comprehensive logging with proper levels
- ‚úÖ **Error Handling** - Graceful error responses with detailed messages
- ‚úÖ **Service Initialization** - Robust model loading with error recovery

**Service Features:**
- Automatic model validation on startup
- Graceful degradation and error recovery
- Real-time service status monitoring
- Resource usage tracking

</details>

<details>
<summary><strong>üéõÔ∏è Modern API Design</strong></summary>

**Flexible endpoint architecture:**
- ‚úÖ **Dual Endpoints** - Both GET and POST (modern) endpoints
- ‚úÖ **Request Validation** - Pydantic models with type-safe input validation
- ‚úÖ **Response Models** - Structured responses with model metadata and token usage
- ‚úÖ **Parameter Customization** - Override generation parameters per request
- ‚úÖ **Constraint Validation** - Proper limits and validation rules

**API Patterns:**
- Modern: `POST /question-answering` (recommended)
- Legacy: `GET /question-answering?question=...` (for quick testing)

</details>

<details>
<summary><strong>üß™ Real Model Testing</strong></summary>

**Comprehensive validation approach:**
- ‚úÖ **Session-scoped Fixtures** - Efficient test execution with shared model loading
- ‚úÖ **Real Service Testing** - Tests use actual model inference, not mocks
- ‚úÖ **Comprehensive Coverage** - All endpoints, parameters, validation, edge cases
- ‚úÖ **Performance Validation** - Ensures real-world functionality works correctly
- ‚úÖ **Hardware Testing** - Validates GPU/CPU configurations

**Testing Philosophy:**
- Tests actual llama-cpp-python integration
- Validates model loading and inference pipelines
- Ensures API contracts match real model behavior
- Provides confidence for production deployment

</details>

<details>
<summary><strong>üîß Easy Setup & Management</strong></summary>

**Streamlined development experience:**
- ‚úÖ **Auto Model Download** - Automatic model fetching with make commands
- ‚úÖ **Optimized Defaults** - Sensible default configurations
- ‚úÖ **Environment Detection** - Auto GPU/CPU detection and configuration
- ‚úÖ **Error Recovery** - Robust handling of model loading failures
- ‚úÖ **Development Tools** - Makefile with common development commands

**Setup Features:**
- One-command environment initialization
- Automatic dependency management
- Smart hardware detection and optimization

</details>

## üì° API Endpoints

### üè• Health & Status
```http
GET /health                 # Service status, model info, GPU config, threads
```

### ü§ñ Question Answering
```http
# Modern POST endpoint (recommended)
POST /question-answering
Content-Type: application/json

{
  "question": "What is the difference between AI and machine learning?",
  "max_tokens": 150,
  "temperature": 0.7
}

# Legacy GET endpoint (quick testing)
GET /question-answering?question=What%20is%20AI&max_tokens=100&temperature=0.5
```

## üõ†Ô∏è Development Commands

<details>
<summary><strong>Available Make Commands</strong></summary>

| Command | Description |
|---------|-------------|
| `make init` | üöÄ Download model and set up environment |
| `make start` | ‚ñ∂Ô∏è Run app in development mode with auto-reload |
| `make test` | üß™ Run comprehensive test suite with real AI |
| `make lint` | üîç Run code quality checks with Ruff |
| `make download` | üì• Download model file only (auto-detects curl/wget) |

</details>

## üîß Configuration & Setup

### Environment Configuration

Create `.env_dev` file:
```bash
# Model configuration
LLM_MODEL="./models/SmolLM2-135M-Instruct-Q4_K_M.gguf"

# Generation parameters
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=100

# Performance tuning
LLM_N_CTX=2048           # Context window size
LLM_N_THREADS=-1         # CPU threads (-1 = auto)
LLM_N_GPU_LAYERS=-1      # GPU layers (-1 = auto, 0 = CPU only)
LLM_VERBOSE=true
LLM_SEED=-1              # Random seed (-1 = random)
```

### Performance Configuration Guide

| Parameter | Description | Default | Notes |
|-----------|-------------|---------|-------|
| `LLM_N_CTX` | Context window size | 2048 | Larger values use more memory |
| `LLM_N_THREADS` | CPU threads | -1 (auto) | Set to number of CPU cores |
| `LLM_N_GPU_LAYERS` | GPU layers | -1 (auto) | 0 = CPU only, higher = more GPU |
| `LLM_SEED` | Random seed | -1 (random) | Set for reproducible outputs |

## üîß Model Management

### Default Model: SmolLM2 135M Instruct

**Optimized for development and learning:**
- **Size:** ~135MB download (Q4_K_M quantized)
- **Memory:** ~200MB RAM usage
- **Performance:** Very fast inference on CPU
- **Quality:** Compact model optimized for efficiency

### Using Different Models

1. **Download your GGUF model** to `./models/` directory
2. **Update configuration:**
   ```bash
   LLM_MODEL="./models/your-model.gguf"
   ```
3. **Adjust settings** for your model:
   ```bash
   LLM_N_CTX=4096        # If your model supports larger context
   LLM_N_GPU_LAYERS=32   # Adjust based on your GPU memory
   ```

### Popular Model Options

| Model | Size | Memory | Use Case |
|-------|------|--------|----------|
| SmolLM2-135M | ~135MB | ~200MB | Development, fast inference |
| SmolLM2-360M | ~360MB | ~500MB | Better quality, still fast |
| Llama-3.2-1B | ~1GB | ~1.5GB | Production quality |
| Llama-3.2-3B | ~3GB | ~4GB | High quality responses |

## üéõÔ∏è Hardware Acceleration

### GPU Support

**Automatic Detection:**
- ‚úÖ **CUDA** - NVIDIA GPUs automatically detected
- ‚úÖ **Metal** - Apple Silicon Macs automatically detected  
- ‚úÖ **CPU Fallback** - Graceful fallback to CPU-only mode

**GPU Configuration:**
```bash
# Auto-detect and use all available GPU layers
LLM_N_GPU_LAYERS=-1

# Use specific number of GPU layers
LLM_N_GPU_LAYERS=20

# CPU-only mode
LLM_N_GPU_LAYERS=0
```

### Performance Optimization

**For CPU-only systems:**
```bash
LLM_N_THREADS=8          # Set to your CPU core count
LLM_N_GPU_LAYERS=0       # Disable GPU
LLM_N_CTX=1024          # Smaller context for less memory
```

**For GPU systems:**
```bash
LLM_N_GPU_LAYERS=-1      # Use all GPU layers
LLM_N_CTX=4096          # Larger context with GPU
LLM_N_THREADS=4         # Fewer CPU threads when using GPU
```

## üß™ Testing Strategy

**Real model validation:**

```bash
make test
# Tests use actual model inference
# First run initializes models (takes longer)
# Subsequent runs are fast (cached models)
```

**What gets tested:**
- ‚úÖ Model loading and initialization
- ‚úÖ Question answering with various parameters
- ‚úÖ Error handling for invalid inputs
- ‚úÖ GPU/CPU configuration validation
- ‚úÖ Context window and threading
- ‚úÖ API contract validation

## üìÅ Project Structure

```
llama/
‚îú‚îÄ‚îÄ main.py              # FastAPI app with llama-cpp-python
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ SmolLM2-135M-Instruct-Q4_K_M.gguf  # Downloaded model
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_main.py     # Real AI integration tests
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ requirements.txt     # Dependencies (llama-cpp-python, etc.)
‚îú‚îÄ‚îÄ .env_dev            # Environment configuration
‚îú‚îÄ‚îÄ Makefile            # Development commands
‚îî‚îÄ‚îÄ README.md           # This file
```

## üéì Local LLM Learning Path

### Mastering Local LLM Inference

1. **üîß Setup & Configuration**
   - Run `make init` to set up everything
   - Check `/health` endpoint for model and hardware info
   - Experiment with different configuration parameters

2. **ü§ñ Test Question Answering**
   - Start with simple questions via GET endpoint
   - Move to POST endpoint with custom parameters
   - Experiment with temperature and token limits

3. **‚ö° Performance Tuning**
   - Monitor memory usage during inference
   - Test CPU vs GPU performance
   - Optimize thread and layer configurations

4. **üîÑ Model Experimentation**
   - Try different GGUF models
   - Compare model sizes vs quality
   - Test context window capabilities

5. **üß™ Run Real Tests**
   - Execute `make test` to see model validation
   - Study test patterns for your own endpoints
   - Understand model loading and inference strategies

## üöÄ Production Deployment

### üîí Security Best Practices

- [ ] Implement API key authentication
- [ ] Add rate limiting for inference endpoints
- [ ] Validate and sanitize all text inputs
- [ ] Set up request/response logging
- [ ] Configure CORS for specific domains
- [ ] Monitor model usage and resource consumption

### ‚ö° Performance Optimization

- [ ] Use appropriate model quantization (Q4_0, Q5_0, Q8_0)
- [ ] Configure optimal GPU layers for your hardware
- [ ] Set thread count to match CPU cores
- [ ] Balance context window with memory constraints
- [ ] Implement request batching for efficiency
- [ ] Add model response caching for common queries

### üõ†Ô∏è Scaling Considerations

- [ ] Plan for model file distribution and caching
- [ ] Set up multiple model instances for load balancing
- [ ] Configure monitoring for memory and GPU usage
- [ ] Implement model warm-up procedures
- [ ] Plan for model updates and versioning
- [ ] Set up backup inference endpoints

## üîÑ Extension Ideas

<details>
<summary><strong>ü§ñ Advanced LLM Features</strong></summary>

**Ready to implement:**
- Multi-turn conversation with context memory
- Custom prompt templates and formatting
- Function calling and tool usage
- JSON schema-constrained outputs
- Streaming responses for long generations

</details>

<details>
<summary><strong>üèóÔ∏è Infrastructure Scaling</strong></summary>

**Production enhancements:**
- Multiple model hosting with model switching
- Load balancing across multiple instances
- Model quantization optimization
- Custom sampling strategies
- Integration with monitoring systems

</details>

<details>
<summary><strong>üîß Model Management</strong></summary>

**Advanced model operations:**
- Automatic model downloading and caching
- Model validation and testing pipelines
- A/B testing between different models
- Model performance benchmarking
- Custom fine-tuned model integration

</details>

## üöÄ Next Steps

### Explore Other AI Templates

- ü§ñ **Comprehensive NLP** - Try the [NLP template](../nlp/README.md) for 8 NLP capabilities
- üîó **LangChain Integration** - Check out [LangChain template](../langchain/README.md) for workflows
- üöÄ **Enterprise Features** - Add auth with [Advanced template](../advanced/README.md)

### Learn More About Local LLMs
- üìö [llama-cpp-python Documentation](https://github.com/abetlen/llama-cpp-python)
- ü¶ô [Llama Model Family](https://ai.meta.com/llama/)
- üîß [GGUF Format Guide](https://huggingface.co/docs/hub/gguf)
- ‚ö° [Quantization Explained](https://huggingface.co/blog/4bit-transformers-bitsandbytes)

---

<div align="center">

**Local LLM power with production-ready patterns** ü¶ô

*Want cloud-based LLMs? Try the [üîó LangChain template](../langchain/README.md)*

</div>