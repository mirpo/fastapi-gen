<div align="center">

# ğŸ¤– NLP - Comprehensive AI Language Processing Template

**Production-ready FastAPI with 8 NLP capabilities using modern Transformers**

*This project was bootstrapped with [FastAPI Gen](https://github.com/mirpo/fastapi-gen)*

[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![Transformers](https://img.shields.io/badge/ğŸ¤—%20Transformers-4.0+-orange.svg)](https://huggingface.co/transformers)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)

</div>

---

## ğŸ¯ What You'll Build

A **comprehensive NLP service** with 8 powerful AI capabilities:

ğŸ§  **8 NLP Capabilities** â†’ Complete text processing pipeline  
ğŸ—ï¸ **Production Architecture** â†’ Startup model loading, device auto-detection  
ğŸ›ï¸ **Smart Configuration** â†’ Environment-based config, multiple models  
âš¡ **Performance Optimized** â†’ Model caching, concurrent handling, hardware acceleration  
ğŸ’Š **Production Monitoring** â†’ Health checks, model status, logging  
ğŸ§ª **Real AI Testing** â†’ Actual model inference validation  

## âš¡ Quick Start in 30 Seconds

```bash
# You're already here! Initialize the environment:
make init  # Sets up Python environment and dependencies

# Start the app:
make start

# The app will download models on first startup (be patient!)
# Open: http://localhost:8000/docs
```

ğŸš€ **Open:** [http://localhost:8000/docs](http://localhost:8000/docs) to see your interactive API documentation.

> â±ï¸ **First startup:** Downloads models (~1-2GB total), takes 3-5 minutes depending on internet speed.

## ğŸ§  8 NLP Capabilities

<details>
<summary><strong>ğŸ“ Text Summarization</strong></summary>

**Advanced text summarization using T5-small:**
- âœ… **Extractive & Abstractive** - Generate concise summaries from long text
- âœ… **Configurable Length** - Control summary length with max_length parameter
- âœ… **Quality Models** - T5-small for balanced speed and quality
- âœ… **Batch Processing** - Handle multiple texts efficiently

**Try it:**
```bash
curl -X POST "http://localhost:8000/summarize" \
  -H "Content-Type: application/json" \
  -d '{"text": "Your long text here...", "max_length": 150}'
```

</details>

<details>
<summary><strong>ğŸ·ï¸ Named Entity Recognition (NER)</strong></summary>

**Extract entities from text:**
- âœ… **Entity Types** - Persons, organizations, locations, and more
- âœ… **Confidence Scores** - Get confidence levels for each entity
- âœ… **Position Tracking** - Know where entities appear in text
- âœ… **Multiple Models** - Choose between speed and accuracy

**Extracted Entities:**
- PER (Person names)
- ORG (Organizations)
- LOC (Locations)
- MISC (Miscellaneous entities)

</details>

<details>
<summary><strong>âœï¸ Text Generation</strong></summary>

**Creative text generation with SmolLM:**
- âœ… **Prompt Completion** - Continue text from given prompts
- âœ… **Parameter Control** - Temperature, max tokens, and more
- âœ… **Efficient Models** - SmolLM for fast generation
- âœ… **Reproducible Output** - Seed control for consistent results

**Generation Controls:**
- Temperature (creativity level)
- Max new tokens (length control)
- Top-p sampling for quality

</details>

<details>
<summary><strong>â“ Question Answering</strong></summary>

**Context-based question answering:**
- âœ… **Extractive QA** - Find answers within provided context
- âœ… **Confidence Scores** - Get answer confidence levels
- âœ… **Position Tracking** - Know where answers are found
- âœ… **DistilBERT Model** - Fast and accurate processing

**Perfect for:**
- Document question answering
- Knowledge base queries
- FAQ automation

</details>

<details>
<summary><strong>ğŸ”¢ Sentence Embeddings</strong></summary>

**Generate dense vector representations:**
- âœ… **Semantic Vectors** - Convert text to numerical representations
- âœ… **Batch Processing** - Handle multiple texts in one request
- âœ… **MiniLM Model** - Optimized for speed and quality
- âœ… **Similarity Ready** - Perfect for semantic search and clustering

**Use Cases:**
- Semantic search
- Document clustering
- Recommendation systems
- Content similarity

</details>

<details>
<summary><strong>ğŸ˜Š Sentiment Analysis</strong></summary>

**Classify text sentiment:**
- âœ… **Binary Classification** - Positive/Negative sentiment
- âœ… **Confidence Scores** - Numerical confidence levels
- âœ… **Fast Processing** - DistilBERT for speed
- âœ… **Production Ready** - Handles various text types

**Applications:**
- Customer feedback analysis
- Social media monitoring
- Product reviews
- Content moderation

</details>

<details>
<summary><strong>ğŸ¯ Zero-shot Classification</strong></summary>

**Classify text without training data:**
- âœ… **Custom Categories** - Define your own classification labels
- âœ… **No Training Required** - Classify into arbitrary categories
- âœ… **Ranked Results** - Get probability scores for all labels
- âœ… **Flexible Input** - Any text, any categories

**Example Use Cases:**
- Content categorization
- Intent classification
- Topic modeling
- Dynamic labeling

</details>

<details>
<summary><strong>ğŸ” Text Similarity</strong></summary>

**Calculate semantic similarity between texts:**
- âœ… **Similarity Scores** - Numerical similarity from 0 to 1
- âœ… **Semantic Understanding** - Beyond keyword matching
- âœ… **Fast Computation** - Optimized embeddings
- âœ… **Pairwise Comparison** - Compare any two texts

**Perfect for:**
- Duplicate detection
- Content matching
- Recommendation systems
- Search ranking

</details>

## ğŸ“¡ API Endpoints

### ğŸ¥ Health & Status
```http
GET /health                 # Service status, device info, model loading status
```

### ğŸ“ Text Processing
```http
# Text Summarization
POST /summarize
{
  "text": "Long text to summarize...",
  "max_length": 150
}

# Named Entity Recognition  
POST /ner
{
  "text": "John Smith works at Microsoft in Seattle."
}

# Text Generation
POST /text-generation
{
  "text": "The future of AI is",
  "max_new_tokens": 100,
  "temperature": 0.7
}

# Question Answering
POST /question-answering
{
  "context": "AI is a field of computer science...",
  "question": "What is AI?"
}
```

### ğŸ”¢ Advanced Analysis
```http
# Sentence Embeddings
POST /embeddings
{
  "texts": ["Hello world", "How are you?"]
}

# Sentiment Analysis
POST /sentiment
{
  "text": "I love this product!"
}

# Zero-shot Classification
POST /classify
{
  "text": "I love programming in Python",
  "candidate_labels": ["technology", "sports", "cooking"]
}

# Text Similarity
POST /similarity
{
  "text1": "The cat is sleeping",
  "text2": "A cat is taking a nap"
}
```

## ğŸ› ï¸ Development Commands

<details>
<summary><strong>Available Make Commands</strong></summary>

| Command | Description |
|---------|-------------|
| `make init` | ğŸš€ Set up Python environment and install dependencies |
| `make start` | â–¶ï¸ Run app in development mode with auto-reload |
| `make test` | ğŸ§ª Run comprehensive test suite with real AI |
| `make lint` | ğŸ” Run code quality checks with Ruff |

</details>

## ğŸ”§ Configuration & Setup

### Environment Configuration

Create `.env_dev` file:
```bash
# Model configurations
SUMMARIZE_MODEL="t5-small"
NER_MODEL="dslim/bert-base-NER"
TEXT_GENERATION_MODEL="HuggingFaceTB/SmolLM-135M"
QA_MODEL="distilbert/distilbert-base-cased-distilled-squad"
EMBEDDING_MODEL="sentence-transformers/all-MiniLM-L6-v2"
SENTIMENT_MODEL="distilbert/distilbert-base-uncased-finetuned-sst-2-english"
ZERO_SHOT_MODEL="typeform/distilbert-base-uncased-mnli"

# Device and performance settings
DEVICE="auto"  # auto, cpu, cuda, mps
MAX_LENGTH=512
TEMPERATURE=0.7
TOP_P=0.9
```

### Model Options & Alternatives

<details>
<summary><strong>ğŸ“ Summarization Models</strong></summary>

| Model | Size | Speed | Quality | Use Case |
|-------|------|-------|---------|----------|
| `t5-small` (default) | 240MB | Fast | Good | Development, balanced |
| `facebook/bart-large-cnn` | 1.6GB | Slower | Excellent | Production, news |
| `google/pegasus-xsum` | 2.3GB | Slowest | Excellent | Academic papers |

</details>

<details>
<summary><strong>ğŸ·ï¸ NER Models</strong></summary>

| Model | Size | Language | Accuracy |
|-------|------|----------|----------|
| `dslim/bert-base-NER` (default) | 400MB | English | High |
| `dslim/bert-large-NER` | 1.3GB | English | Higher |
| `dbmdz/bert-large-cased-finetuned-conll03-english` | 1.3GB | English | Very High |

</details>

<details>
<summary><strong>âœï¸ Text Generation Models</strong></summary>

| Model | Size | Speed | Quality |
|-------|------|-------|---------|
| `SmolLM-135M` (default) | 270MB | Very Fast | Good |
| `openai-community/gpt2` | 500MB | Fast | Better |
| `microsoft/DialoGPT-small` | 345MB | Fast | Conversational |

</details>

### Hardware Requirements

- **Minimum:** 8GB RAM, CPU-only operation
- **Recommended:** 16GB+ RAM, GPU with 4GB+ VRAM
- **GPU Support:** 
  - CUDA (NVIDIA) automatically detected
  - MPS (Apple Silicon) excellent performance
  - CPU fallback always available

## ğŸ—ï¸ Production Architecture

### Smart Model Loading

**Optimized startup patterns:**
- âœ… **Startup Loading** - All models load during app startup (not on first request)
- âœ… **Device Detection** - Automatic GPU/CPU/MPS detection and utilization
- âœ… **Memory Management** - Models stay in memory for application lifetime
- âœ… **Concurrent Handling** - Single model instances handle multiple requests
- âœ… **Error Recovery** - Graceful handling of model loading failures

### Performance Characteristics

| Phase | Duration | Memory Usage | Notes |
|-------|----------|--------------|-------|
| **Cold Start** | 30-60s | ~2-4GB | Model download and loading |
| **Warm Inference** | <1s | ~2-4GB | Most operations under 1 second |
| **GPU Acceleration** | <500ms | 2-6GB VRAM | When available |
| **Concurrent Requests** | Parallel | Shared | Multiple requests use same models |

## ğŸ§ª Testing Philosophy

**Real AI validation approach:**

```bash
make test
# Tests use actual model inference (not mocks)
# Automatically uses CPU in CI environments
# First run downloads models (takes longer)
# Subsequent runs are fast (cached models)
```

**Comprehensive test coverage:**
- âœ… **Model Loading** - Validates all 8 models load correctly
- âœ… **Inference Testing** - Tests actual AI capabilities
- âœ… **Cross-platform** - CPU/GPU/MPS compatibility
- âœ… **Error Handling** - Invalid inputs and edge cases
- âœ… **Performance** - Response times and memory usage
- âœ… **API Contracts** - Request/response validation

### CI/CD Integration

**GitHub Actions ready:**
```yaml
env:
  CI: true          # Automatically detected
  DEVICE: cpu       # Forces CPU usage in CI
```

The service automatically detects CI environments and switches to CPU mode to avoid memory issues.

## ğŸ“ Project Structure

```
nlp/
â”œâ”€â”€ main.py              # FastAPI app with 8 NLP capabilities
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_main.py     # Real AI integration tests
â”‚   â”œâ”€â”€ conftest.py      # Test configuration and fixtures
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ requirements.txt     # Dependencies (transformers, torch, etc.)
â”œâ”€â”€ .env_dev            # Environment configuration
â”œâ”€â”€ Makefile            # Development commands
â””â”€â”€ README.md           # This file
```

## ğŸ“ NLP Learning Path

### Mastering AI Text Processing

1. **ğŸ”§ Setup & Configuration**
   - Run `make init` to set up environment
   - Check `/health` endpoint for model status
   - Experiment with different models in configuration

2. **ğŸ“ Text Processing Basics**
   - Try text summarization with different lengths
   - Extract entities from various text types
   - Generate text with different creativity levels

3. **â“ Advanced Analysis**
   - Test question answering with different contexts
   - Analyze sentiment of various text samples
   - Create embeddings for semantic search

4. **ğŸ¯ Classification & Similarity**
   - Use zero-shot classification for custom categories
   - Calculate text similarity for content matching
   - Combine multiple capabilities for complex workflows

5. **âš¡ Performance Optimization**
   - Monitor memory usage across models
   - Test GPU vs CPU performance
   - Optimize batch processing for efficiency

6. **ğŸ§ª Real Testing**
   - Run `make test` to see AI validation
   - Study test patterns for production usage
   - Understand model loading and inference strategies

## ğŸš€ Production Deployment

### ğŸ”’ Security Best Practices

- [ ] Implement API key authentication for all endpoints
- [ ] Add rate limiting based on model complexity
- [ ] Validate and sanitize all text inputs
- [ ] Set up comprehensive request/response logging
- [ ] Configure CORS for specific domains
- [ ] Monitor model usage and associated costs

### âš¡ Performance Optimization

- [ ] Choose optimal models for your use case (speed vs quality)
- [ ] Use GPU acceleration when available
- [ ] Implement request batching for similar operations
- [ ] Add intelligent caching for frequent requests
- [ ] Monitor memory usage and optimize model selection
- [ ] Consider model quantization for production speed

### ğŸ› ï¸ Scaling Considerations

- [ ] Set up horizontal scaling with multiple instances
- [ ] Implement load balancing across model servers
- [ ] Configure monitoring for all 8 NLP capabilities
- [ ] Plan for model updates and versioning
- [ ] Set up backup inference endpoints
- [ ] Integrate with model monitoring and analytics tools

## ğŸ”„ Extension Ideas

<details>
<summary><strong>ğŸ§  Advanced NLP Features</strong></summary>

**Ready to implement:**
- Multi-language support with language detection
- Custom fine-tuned models for domain-specific tasks
- Document processing with OCR integration
- Speech-to-text and text-to-speech capabilities
- Advanced prompt engineering and templates

</details>

<details>
<summary><strong>ğŸ” Search & Retrieval</strong></summary>

**AI-powered search features:**
- Vector database integration (Pinecone, Weaviate)
- Semantic search with embeddings
- Retrieval Augmented Generation (RAG)
- Document indexing and search
- Similarity-based recommendation systems

</details>

<details>
<summary><strong>ğŸ“Š Analytics & Insights</strong></summary>

**Business intelligence features:**
- Text analytics dashboards
- Trend analysis and topic modeling
- Automated content categorization
- Sentiment tracking over time
- Custom classification for business domains

</details>

## ğŸš€ Next Steps

### Explore Other AI Templates

- ğŸ”— **LLM Integration** - Try the [LangChain template](../langchain/README.md) for conversational AI
- ğŸ¦™ **Local LLM** - Check out [Llama template](../llama/README.md) for local inference
- ğŸš€ **Enterprise Features** - Add auth with [Advanced template](../advanced/README.md)

### Learn More About NLP
- ğŸ“š [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- ğŸ“ [NLP Course](https://huggingface.co/course/chapter1/1)
- ğŸ”¬ [Sentence Transformers](https://www.sbert.net/)
- âš¡ [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)

---

<div align="center">

**8 powerful NLP capabilities in one production-ready service** ğŸ¤–

*Want conversational AI? Try the [ğŸ”— LangChain template](../langchain/README.md)*

</div>