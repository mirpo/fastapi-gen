<div align="center">

# NLP - Comprehensive AI Language Processing Template

**Production-ready FastAPI with 8 NLP capabilities using modern Transformers**

*This project was bootstrapped with [FastAPI Gen](https://github.com/mirpo/fastapi-gen)*

[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![Transformers](https://img.shields.io/badge/ü§ó%20Transformers-4.0+-orange.svg)](https://huggingface.co/transformers)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)

</div>

---

## What You'll Build

A **comprehensive NLP service** with 8 powerful AI capabilities:

**8 NLP Capabilities** ‚Üí Complete text processing pipeline  
**Production Architecture** ‚Üí Startup model loading, device auto-detection  
**Smart Configuration** ‚Üí Environment-based config, multiple models  
**Performance Optimized** ‚Üí Model caching, concurrent handling, hardware acceleration  
**Production Monitoring** ‚Üí Health checks, model status, logging  
**Real AI Testing** ‚Üí Actual model inference validation  

## Quick Start in 30 Seconds

```bash
# You're already here! Initialize the environment:
make init  # Sets up Python environment and dependencies

# Start the app:
make start

# The app will download models on first startup (be patient!)
# Open: http://localhost:8000/docs
```

**Open:** [http://localhost:8000/docs](http://localhost:8000/docs) to see your interactive API documentation.

> **First startup:** Downloads models (~1-2GB total), takes 3-5 minutes depending on internet speed.

## 8 NLP Capabilities

<details>
<summary><strong>Text Summarization</strong></summary>

**Advanced text summarization using T5-small:**
- ‚úÖ **Extractive & Abstractive** - Generate concise summaries from long text
- ‚úÖ **Configurable Length** - Control summary length with max_length parameter
- ‚úÖ **Quality Models** - T5-small for balanced speed and quality
- ‚úÖ **Batch Processing** - Handle multiple texts efficiently

**Try it:**
```bash
curl -X POST "http://localhost:8000/summarize" \
  -H "Content-Type: application/json" \
  -d '{"text": "Your long text here...", "max_length": 150}'
```

</details>

<details>
<summary><strong>Named Entity Recognition (NER)</strong></summary>

**Extract entities from text:**
- ‚úÖ **Entity Types** - Persons, organizations, locations, and more
- ‚úÖ **Confidence Scores** - Get confidence levels for each entity
- ‚úÖ **Position Tracking** - Know where entities appear in text
- ‚úÖ **Multiple Models** - Choose between speed and accuracy

**Extracted Entities:**
- PER (Person names)
- ORG (Organizations)
- LOC (Locations)
- MISC (Miscellaneous entities)

</details>

<details>
<summary><strong>Text Generation</strong></summary>

**Creative text generation with SmolLM:**
- ‚úÖ **Prompt Completion** - Continue text from given prompts
- ‚úÖ **Parameter Control** - Temperature, max tokens, and more
- ‚úÖ **Efficient Models** - SmolLM for fast generation
- ‚úÖ **Reproducible Output** - Seed control for consistent results

**Generation Controls:**
- Temperature (creativity level)
- Max new tokens (length control)
- Top-p sampling for quality

</details>

<details>
<summary><strong>Question Answering</strong></summary>

**Context-based question answering:**
- ‚úÖ **Extractive QA** - Find answers within provided context
- ‚úÖ **Confidence Scores** - Get answer confidence levels
- ‚úÖ **Position Tracking** - Know where answers are found
- ‚úÖ **DistilBERT Model** - Fast and accurate processing

**Perfect for:**
- Document question answering
- Knowledge base queries
- FAQ automation

</details>

<details>
<summary><strong>Sentence Embeddings</strong></summary>

**Generate dense vector representations:**
- ‚úÖ **Semantic Vectors** - Convert text to numerical representations
- ‚úÖ **Batch Processing** - Handle multiple texts in one request
- ‚úÖ **MiniLM Model** - Optimized for speed and quality
- ‚úÖ **Similarity Ready** - Perfect for semantic search and clustering

**Use Cases:**
- Semantic search
- Document clustering
- Recommendation systems
- Content similarity

</details>

<details>
<summary><strong>Sentiment Analysis</strong></summary>

**Classify text sentiment:**
- ‚úÖ **Binary Classification** - Positive/Negative sentiment
- ‚úÖ **Confidence Scores** - Numerical confidence levels
- ‚úÖ **Fast Processing** - DistilBERT for speed
- ‚úÖ **Production Ready** - Handles various text types

**Applications:**
- Customer feedback analysis
- Social media monitoring
- Product reviews
- Content moderation

</details>

<details>
<summary><strong>Zero-shot Classification</strong></summary>

**Classify text without training data:**
- ‚úÖ **Custom Categories** - Define your own classification labels
- ‚úÖ **No Training Required** - Classify into arbitrary categories
- ‚úÖ **Ranked Results** - Get probability scores for all labels
- ‚úÖ **Flexible Input** - Any text, any categories

**Example Use Cases:**
- Content categorization
- Intent classification
- Topic modeling
- Dynamic labeling

</details>

<details>
<summary><strong>Text Similarity</strong></summary>

**Calculate semantic similarity between texts:**
- ‚úÖ **Similarity Scores** - Numerical similarity from 0 to 1
- ‚úÖ **Semantic Understanding** - Beyond keyword matching
- ‚úÖ **Fast Computation** - Optimized embeddings
- ‚úÖ **Pairwise Comparison** - Compare any two texts

**Perfect for:**
- Duplicate detection
- Content matching
- Recommendation systems
- Search ranking

</details>

## API Endpoints

### Health & Status
```http
GET /health                 # Service status, device info, model loading status
```

### Text Processing
```http
# Text Summarization
POST /summarize
{
  "text": "Long text to summarize...",
  "max_length": 150
}

# Named Entity Recognition  
POST /ner
{
  "text": "John Smith works at Microsoft in Seattle."
}

# Text Generation
POST /text-generation
{
  "text": "The future of AI is",
  "max_new_tokens": 100,
  "temperature": 0.7
}

# Question Answering
POST /question-answering
{
  "context": "AI is a field of computer science...",
  "question": "What is AI?"
}
```

### Advanced Analysis
```http
# Sentence Embeddings
POST /embeddings
{
  "texts": ["Hello world", "How are you?"]
}

# Sentiment Analysis
POST /sentiment
{
  "text": "I love this product!"
}

# Zero-shot Classification
POST /classify
{
  "text": "I love programming in Python",
  "candidate_labels": ["technology", "sports", "cooking"]
}

# Text Similarity
POST /similarity
{
  "text1": "The cat is sleeping",
  "text2": "A cat is taking a nap"
}
```

## Development Commands

<details>
<summary><strong>Available Make Commands</strong></summary>

| Command | Description |
|---------|-------------|
| `make init` | Set up Python environment and install dependencies |
| `make start` | Run app in development mode with auto-reload |
| `make test` | Run comprehensive test suite with real AI |
| `make lint` | Run code quality checks with Ruff |

</details>

## Configuration & Setup

### Environment Configuration

Create `.env_dev` file:
```bash
# Model configurations
SUMMARIZE_MODEL="t5-small"
NER_MODEL="dslim/bert-base-NER"
TEXT_GENERATION_MODEL="HuggingFaceTB/SmolLM-135M"
QA_MODEL="distilbert/distilbert-base-cased-distilled-squad"
EMBEDDING_MODEL="sentence-transformers/all-MiniLM-L6-v2"
SENTIMENT_MODEL="distilbert/distilbert-base-uncased-finetuned-sst-2-english"
ZERO_SHOT_MODEL="typeform/distilbert-base-uncased-mnli"

# Device and performance settings
DEVICE="auto"  # auto, cpu, cuda, mps
MAX_LENGTH=512
TEMPERATURE=0.7
TOP_P=0.9
```

### Model Options & Alternatives

<details>
<summary><strong>üìù Summarization Models</strong></summary>

| Model | Size | Speed | Quality | Use Case |
|-------|------|-------|---------|----------|
| `t5-small` (default) | 240MB | Fast | Good | Development, balanced |
| `facebook/bart-large-cnn` | 1.6GB | Slower | Excellent | Production, news |
| `google/pegasus-xsum` | 2.3GB | Slowest | Excellent | Academic papers |

</details>

<details>
<summary><strong>üè∑Ô∏è NER Models</strong></summary>

| Model | Size | Language | Accuracy |
|-------|------|----------|----------|
| `dslim/bert-base-NER` (default) | 400MB | English | High |
| `dslim/bert-large-NER` | 1.3GB | English | Higher |
| `dbmdz/bert-large-cased-finetuned-conll03-english` | 1.3GB | English | Very High |

</details>

<details>
<summary><strong>‚úçÔ∏è Text Generation Models</strong></summary>

| Model | Size | Speed | Quality |
|-------|------|-------|---------|
| `SmolLM-135M` (default) | 270MB | Very Fast | Good |
| `openai-community/gpt2` | 500MB | Fast | Better |
| `microsoft/DialoGPT-small` | 345MB | Fast | Conversational |

</details>

### Hardware Requirements

- **Minimum:** 8GB RAM, CPU-only operation
- **Recommended:** 16GB+ RAM, GPU with 4GB+ VRAM
- **GPU Support:** 
  - CUDA (NVIDIA) automatically detected
  - MPS (Apple Silicon) excellent performance
  - CPU fallback always available

## Production Architecture

### Smart Model Loading

**Optimized startup patterns:**
- ‚úÖ **Startup Loading** - All models load during app startup (not on first request)
- ‚úÖ **Device Detection** - Automatic GPU/CPU/MPS detection and utilization
- ‚úÖ **Memory Management** - Models stay in memory for application lifetime
- ‚úÖ **Concurrent Handling** - Single model instances handle multiple requests
- ‚úÖ **Error Recovery** - Graceful handling of model loading failures

### Performance Characteristics

| Phase | Duration | Memory Usage | Notes |
|-------|----------|--------------|-------|
| **Cold Start** | 30-60s | ~2-4GB | Model download and loading |
| **Warm Inference** | <1s | ~2-4GB | Most operations under 1 second |
| **GPU Acceleration** | <500ms | 2-6GB VRAM | When available |
| **Concurrent Requests** | Parallel | Shared | Multiple requests use same models |

## Testing Philosophy

**Real AI validation approach:**

```bash
make test
# Tests use actual model inference (not mocks)
# Automatically uses CPU in CI environments
# First run downloads models (takes longer)
# Subsequent runs are fast (cached models)
```

**Comprehensive test coverage:**
- ‚úÖ **Model Loading** - Validates all 8 models load correctly
- ‚úÖ **Inference Testing** - Tests actual AI capabilities
- ‚úÖ **Cross-platform** - CPU/GPU/MPS compatibility
- ‚úÖ **Error Handling** - Invalid inputs and edge cases
- ‚úÖ **Performance** - Response times and memory usage
- ‚úÖ **API Contracts** - Request/response validation

### CI/CD Integration

**GitHub Actions ready:**
```yaml
env:
  CI: true          # Automatically detected
  DEVICE: cpu       # Forces CPU usage in CI
```

The service automatically detects CI environments and switches to CPU mode to avoid memory issues.

## Project Structure

```
nlp/
‚îú‚îÄ‚îÄ main.py              # FastAPI app with 8 NLP capabilities
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_main.py     # Real AI integration tests
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py      # Test configuration and fixtures
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ requirements.txt     # Dependencies (transformers, torch, etc.)
‚îú‚îÄ‚îÄ .env_dev            # Environment configuration
‚îú‚îÄ‚îÄ Makefile            # Development commands
‚îî‚îÄ‚îÄ README.md           # This file
```

## NLP Learning Path

### Mastering AI Text Processing

1. **Setup & Configuration**
   - Run `make init` to set up environment
   - Check `/health` endpoint for model status
   - Experiment with different models in configuration

2. **Text Processing Basics**
   - Try text summarization with different lengths
   - Extract entities from various text types
   - Generate text with different creativity levels

3. **Advanced Analysis**
   - Test question answering with different contexts
   - Analyze sentiment of various text samples
   - Create embeddings for semantic search

4. **Classification & Similarity**
   - Use zero-shot classification for custom categories
   - Calculate text similarity for content matching
   - Combine multiple capabilities for complex workflows

5. **Performance Optimization**
   - Monitor memory usage across models
   - Test GPU vs CPU performance
   - Optimize batch processing for efficiency

6. **Real Testing**
   - Run `make test` to see AI validation
   - Study test patterns for production usage
   - Understand model loading and inference strategies

## Production Deployment

### Security Best Practices

- [ ] Implement API key authentication for all endpoints
- [ ] Add rate limiting based on model complexity
- [ ] Validate and sanitize all text inputs
- [ ] Set up comprehensive request/response logging
- [ ] Configure CORS for specific domains
- [ ] Monitor model usage and associated costs

### Performance Optimization

- [ ] Choose optimal models for your use case (speed vs quality)
- [ ] Use GPU acceleration when available
- [ ] Implement request batching for similar operations
- [ ] Add intelligent caching for frequent requests
- [ ] Monitor memory usage and optimize model selection
- [ ] Consider model quantization for production speed

### Scaling Considerations

- [ ] Set up horizontal scaling with multiple instances
- [ ] Implement load balancing across model servers
- [ ] Configure monitoring for all 8 NLP capabilities
- [ ] Plan for model updates and versioning
- [ ] Set up backup inference endpoints
- [ ] Integrate with model monitoring and analytics tools

## Extension Ideas

<details>
<summary><strong>Advanced NLP Features</strong></summary>

**Ready to implement:**
- Multi-language support with language detection
- Custom fine-tuned models for domain-specific tasks
- Document processing with OCR integration
- Speech-to-text and text-to-speech capabilities
- Advanced prompt engineering and templates

</details>

<details>
<summary><strong>Search & Retrieval</strong></summary>

**AI-powered search features:**
- Vector database integration (Pinecone, Weaviate)
- Semantic search with embeddings
- Retrieval Augmented Generation (RAG)
- Document indexing and search
- Similarity-based recommendation systems

</details>

<details>
<summary><strong>Analytics & Insights</strong></summary>

**Business intelligence features:**
- Text analytics dashboards
- Trend analysis and topic modeling
- Automated content categorization
- Sentiment tracking over time
- Custom classification for business domains

</details>

## Next Steps

### Explore Other AI Templates

- **LLM Integration** - Try the [LangChain template](../langchain/README.md) for conversational AI
- **Local LLM** - Check out [Llama template](../llama/README.md) for local inference
- **Enterprise Features** - Add auth with [Advanced template](../advanced/README.md)

### Learn More About NLP
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [NLP Course](https://huggingface.co/course/chapter1/1)
- [Sentence Transformers](https://www.sbert.net/)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)

---

<div align="center">

**8 powerful NLP capabilities in one production-ready service**

*Want conversational AI? Try the [LangChain template](../langchain/README.md)*

</div>