<div align="center">

# LangChain - Modern LLM Integration Template

**Production-ready FastAPI with LangChain for LLM workflows**

*This project was bootstrapped with [FastAPI Gen](https://github.com/mirpo/fastapi-gen)*

[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![LangChain](https://img.shields.io/badge/LangChain-0.3+-purple.svg)](https://python.langchain.com)
[![Transformers](https://img.shields.io/badge/ðŸ¤—%20Transformers-4.0+-orange.svg)](https://huggingface.co/transformers)

</div>

---

## What You'll Build

A **modern LangChain-powered FastAPI service** with production-ready LLM integration:

**Optimized Loading** â†’ Models load at startup, not per request  
**Modern Patterns** â†’ Latest LangChain 0.3.x best practices  
**Smart Configuration** â†’ Auto device detection (CPU/GPU)  
**Dual Endpoints** â†’ Text generation & question answering  
**Production Ready** â†’ Health checks, monitoring, error handling  
**Real AI Testing** â†’ Actual model inference validation  

## Quick Start in 30 Seconds

```bash
# You're already here! Just run:
make start

# The app will download models on first startup (be patient!)
# Then open: http://localhost:8000/docs
```

**Open:** [http://localhost:8000/docs](http://localhost:8000/docs) to see your interactive API documentation.

> **First startup:** Downloads models (~1-2GB), takes 2-3 minutes depending on internet speed.

## LangChain Features

<details>
<summary><strong>Optimized Model Loading</strong></summary>

**Smart startup patterns:**
- âœ… **Startup Caching** - Models load once during app startup
- âœ… **Memory Efficient** - Reuses tokenizer and model instances
- âœ… **Error Handling** - Graceful initialization with detailed logging
- âœ… **Device Detection** - Automatic GPU/CPU selection
- âœ… **Connection Pooling** - Efficient model instance management

**Performance Benefits:**
- First request: Instant (models already loaded)
- Memory usage: Optimized for concurrent requests
- Startup time: 30-60 seconds (one-time model download)

</details>

<details>
<summary><strong>Modern LangChain Patterns</strong></summary>

**Latest LangChain 0.3.x features:**
- âœ… **Updated Imports** - Uses `langchain-huggingface` (not deprecated `langchain-community`)
- âœ… **Current API** - Modern LangChain patterns and method calls
- âœ… **Type Safety** - Full Pydantic integration for requests/responses
- âœ… **Error Handling** - Proper exception management
- âœ… **Async Support** - Non-blocking operations where possible

**Modern Code Structure:**
```python
from langchain_huggingface import HuggingFacePipeline
# Latest imports, not legacy ones
```

</details>

<details>
<summary><strong>Smart Configuration</strong></summary>

**Intelligent setup:**
- âœ… **Auto Device Detection** - GPU when available, CPU fallback
- âœ… **Environment Config** - `.env_dev` and `.env_prod` support
- âœ… **Model Selection** - Configurable model paths and parameters
- âœ… **Generation Settings** - Temperature, max tokens, and more
- âœ… **Runtime Overrides** - Request-level parameter customization

**Configuration Options:**
```bash
TEXT_GENERATION_MODEL="HuggingFaceTB/SmolLM2-360M-Instruct"
DEVICE="auto"  # auto, cpu, or cuda
MAX_NEW_TOKENS=100
TEMPERATURE=0.7
```

</details>

<details>
<summary><strong>Dual AI Endpoints</strong></summary>

**Two complementary LLM use cases:**
- âœ… **Text Generation** - Creative text completion and generation
- âœ… **Question Answering** - Context-based question answering
- âœ… **Flexible Input** - Both GET and POST endpoints
- âœ… **Parameter Control** - Override defaults per request
- âœ… **Structured Output** - Consistent response formats

**Endpoint Types:**
- Modern: `POST /text-generation` (recommended)
- Legacy: `GET /text-generation?text=...` (for quick testing)

</details>

<details>
<summary><strong>Production Monitoring</strong></summary>

**Enterprise-ready observability:**
- âœ… **Health Endpoint** - `/health` with model status and device info
- âœ… **Structured Logging** - Comprehensive logging with proper levels
- âœ… **Error Responses** - Detailed error messages for debugging
- âœ… **Service Status** - Real-time model and configuration information
- âœ… **Performance Metrics** - Ready for monitoring integration

**Health Check Data:**
- Model initialization status
- Device configuration (CPU/GPU)
- Available memory and resources
- Service uptime and readiness

</details>

<details>
<summary><strong>Real AI Testing</strong></summary>

**Comprehensive test strategy:**
- âœ… **Integration Tests** - Tests use actual model inference (not mocks)
- âœ… **Endpoint Coverage** - All endpoints, parameters, and edge cases tested
- âœ… **Performance Validation** - Ensures real-world functionality
- âœ… **Session Fixtures** - Efficient test execution with shared model loading
- âœ… **Error Testing** - Invalid input and edge case handling

**Why Real Testing:**
- Validates actual LangChain integration works
- Ensures API contracts match real model behavior
- Catches model loading and inference issues
- Provides confidence for production deployment

</details>

## API Endpoints

### Health & Status
```http
GET /health                 # Service status, model info, device config
```

### Text Generation
```http
# Modern POST endpoint (recommended)
POST /text-generation
Content-Type: application/json

{
  "text": "The future of artificial intelligence is",
  "max_new_tokens": 100,
  "temperature": 0.7
}

# Legacy GET endpoint (quick testing)
GET /text-generation?text=What%20is%20AI&max_new_tokens=50
```

### Question Answering
```http
# Modern POST endpoint (recommended)  
POST /question-answering
Content-Type: application/json

{
  "context": "AI is a field of computer science focused on creating intelligent machines.",
  "question": "What is AI?",
  "max_new_tokens": 50
}

# Legacy GET endpoint (quick testing)
GET /question-answering?context=AI%20is...&question=What%20is%20AI?
```

## Development Commands

<details>
<summary><strong>Available Make Commands</strong></summary>

| Command | Description |
|---------|-------------|
| `make start` | Run app in development mode with auto-reload |
| `make test` | Run comprehensive test suite with real AI |
| `make lint` | Run code quality checks with Ruff |

</details>

## Configuration & Setup

### Environment Configuration

Create `.env_dev` file:
```bash
# Model and device settings
TEXT_GENERATION_MODEL="HuggingFaceTB/SmolLM2-360M-Instruct"
DEVICE="auto"  # auto, cpu, cuda

# Generation parameters
MAX_NEW_TOKENS=100
MAX_LENGTH=150
TEMPERATURE=0.7
RANDOM_SEED=42
```

### Model Options

| Model | Size | Use Case | Memory |
|-------|------|----------|--------|
| `SmolLM2-360M-Instruct` | 360M | Fast inference, development | ~1GB |
| `SmolLM2-1.7B-Instruct` | 1.7B | Better quality, production | ~3GB |
| `microsoft/DialoGPT-small` | 345M | Conversational AI | ~1GB |

### Hardware Requirements

- **Minimum:** 4GB RAM, CPU-only operation
- **Recommended:** 8GB+ RAM, GPU with 2GB+ VRAM
- **GPU Support:** CUDA (NVIDIA) automatically detected
- **Apple Silicon:** Excellent performance with MPS

## Testing Philosophy

**Real AI validation approach:**

```bash
make test
# Tests download and use actual models
# First run takes longer (model download)
# Subsequent runs are fast (cached models)
```

**What gets tested:**
- âœ… Model loading and initialization
- âœ… Text generation with various parameters  
- âœ… Question answering accuracy
- âœ… Error handling for invalid inputs
- âœ… Device selection and configuration
- âœ… API contract validation

## Project Structure

```
langchain/
â”œâ”€â”€ main.py              # FastAPI app with LangChain integration
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_main.py     # Real AI integration tests
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ requirements.txt     # Dependencies (LangChain, transformers, etc.)
â”œâ”€â”€ .env_dev            # Environment configuration
â”œâ”€â”€ Makefile            # Development commands
â””â”€â”€ README.md           # This file
```

## LangChain Learning Path

### Mastering LLM Integration

1. **Explore Configuration**
   - Check `/health` endpoint for model and device info
   - Try different models in `.env_dev`
   - Experiment with generation parameters

2. **Test Text Generation**
   - Start with simple prompts via GET endpoint
   - Move to POST endpoint with custom parameters
   - Experiment with temperature and token limits

3. **Master Question Answering**
   - Provide context and ask specific questions
   - Try different context lengths
   - Compare answers with different models

4. **Performance Tuning**
   - Monitor memory usage during inference
   - Test concurrent requests
   - Compare CPU vs GPU performance

5. **Run Real Tests**
   - Execute `make test` to see AI validation
   - Study test patterns for your own endpoints
   - Understand model loading strategies

## Production Deployment

### Security Best Practices

- [ ] Implement API key authentication
- [ ] Add rate limiting for inference endpoints
- [ ] Validate and sanitize all text inputs
- [ ] Set up request/response logging
- [ ] Configure CORS for specific domains
- [ ] Monitor model usage and costs

### Performance Optimization

- [ ] Use GPU acceleration when available
- [ ] Implement request batching for efficiency
- [ ] Add model response caching
- [ ] Set up model serving with multiple workers
- [ ] Monitor memory usage and optimize
- [ ] Consider model quantization for speed

### Scaling Considerations

- [ ] Set up multiple model instances
- [ ] Implement load balancing
- [ ] Add model warm-up procedures
- [ ] Configure monitoring and alerting
- [ ] Plan for model updates and versioning
- [ ] Set up backup inference endpoints

## Extension Ideas

<details>
<summary><strong>Advanced LLM Features</strong></summary>

**Ready to implement:**
- Conversation memory and context
- Multi-turn dialogue systems
- Custom fine-tuned models
- Prompt template management
- Response streaming for long outputs

</details>

<details>
<summary><strong>LangChain Ecosystem</strong></summary>

**LangChain integrations:**
- Vector stores for document QA
- Agent systems with tool usage
- Chain composition and workflows
- Memory management systems
- Custom retrieval augmented generation (RAG)

</details>

<details>
<summary><strong>Production Features</strong></summary>

**Enterprise enhancements:**
- Model A/B testing frameworks
- Usage analytics and billing
- Multi-model inference endpoints
- Custom model serving pipelines
- Integration with ML monitoring tools

</details>

## Next Steps

### Explore Other AI Templates

- **Text Processing** - Try the [NLP template](../nlp/README.md) for 8 NLP capabilities
- **Local LLM** - Check out [Llama template](../llama/README.md) for local inference
- **Enterprise Features** - Add auth with [Advanced template](../advanced/README.md)

### Learn More LangChain
- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)
- [LangChain Tutorials](https://python.langchain.com/docs/tutorials/)
- [LangChain Community](https://github.com/langchain-ai/langchain)

---

<div align="center">

**Modern LangChain integration with production-ready patterns**

*Want local LLM control? Try the [Llama template](../llama/README.md)*

</div>